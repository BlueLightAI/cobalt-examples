{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "<img style=\"width:40%;max-width:600px\" alt=\"Bluelight AI Logo\" href=\"https://bluelightai.com/\" src=\"https://github.com/BlueLightAI/cobalt-examples/blob/main/assets/blai-logo-light.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cobalt UI Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://bluelightai.com/contact\">Give Feedback</a> | <a href=\"https://bluelightai.com/\">Our Website</a> | <a href=\"https://bluelightai.com/blog\">Our Blog</a> | <a href=\"https://docs.cobalt.bluelightai.com/\">Cobalt Docs</a> | <a href=\"https://join.slack.com/t/bluelightaicom/shared_invite/zt-2uj0iu5lh-5WgutuwH82RxAOwuq8ptqg\">Slack Community</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tags:** #blai #python #cobaltai #tda #embedding #cobaltui #imageclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last update:** 2024-12-12 (Created: 2024-11-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you will see:\n",
    "\n",
    "- Download the imagenette data (with 10 classes),\n",
    "- Generate embedding using a Clip model for the data,\n",
    "- Visualize the embedded results using a powerful visualization tool, \n",
    "- Generate insights using the visual analysis, and \n",
    "- Select subgroups from the UI and interact with selected subgroup using python code.\n",
    "\n",
    "### You will learn:\n",
    "\n",
    "- Introduction to Cobalt and its UI, and\n",
    "- Application of Cobalt with embedding and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>References:</u>\n",
    "- [Intelligent Search Results for Ecommerce: BluelightAI and Marqo Join Forces](https://bluelightai.com/blog/ecommerce-industry-partnership-with-marqo)\n",
    "- [Curate Your Datasets with Cobalt for Higher Performing Models](https://bluelightai.com/blog/bluelightai-cobalt-control-your-datasets-with-topological-data-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install the CLIP model and PyTorch to create embeddings from some sample data. We follow the setup instructions from the OpenAI's [CLIP](https://github.com/openai/CLIP) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install ftfy packaging regex tqdm torch torchvision\n",
    "# %pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now dowanload the cobalt package from PyPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install cobalt-ai\n",
    "# %pip install --upgrade cobalt-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, lets download the Imagenette dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# torchvision.datasets.Imagenette(root=\".\", split=\"val\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will download data in the current working directory for the notebook. If you want to download it somewhere else, you can specify the root argument with a different path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cobalt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not registered Cobalt yet, please uncomment and run the following to register:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cobalt.register_license()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set up the device to run the CLIP model on. We'll use a GPU if available, otherwise we'll fall back to using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [Imagenette dataset](https://github.com/fastai/imagenette), a subset of the ImageNet dataset on 10 very different classes. (See the dependencies section above for download command.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has is split in two parts, training (9,469 images) and validation (3,925 images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Imagenette\n",
       "    Number of datapoints: 9469\n",
       "    Root location: ."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If this gives an error, please see the dependencies section to download the data\n",
    "torchvision.datasets.Imagenette(root=\".\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Imagenette\n",
       "    Number of datapoints: 3925\n",
       "    Root location: ."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.datasets.Imagenette(root=\".\", split=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this introduction to Cobalt UI, we will use smaller validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imagenette_training = torchvision.datasets.Imagenette(root=\".\", split=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us create a list of file paths for all the images in the dataset in the variable `training_images_paths`, and also note the `training_class_indices` for each of these image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_root = imagenette_training._image_root\n",
    "\n",
    "# Create a list of image paths and corresponding target indices.\n",
    "training_images_paths = []\n",
    "training_class_indices = []\n",
    "for c in os.listdir(training_images_root):\n",
    "    class_path = os.path.join(training_images_root, c)\n",
    "    image_index = imagenette_training.wnid_to_idx[c]\n",
    "    for f in os.listdir(class_path):\n",
    "        training_images_paths.append(os.path.join(class_path, f))\n",
    "        training_class_indices.append(image_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 3925\n"
     ]
    }
   ],
   "source": [
    "print(f'Total files: {len(training_images_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imagenette2/val/n03394916/n03394916_32422.JPEG',\n",
       " 'imagenette2/val/n03394916/n03394916_69132.JPEG',\n",
       " 'imagenette2/val/n03394916/n03394916_33771.JPEG']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_images_paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_class_indices[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these class indices are in numeric form. \n",
    "\n",
    "Imagenette data provides human readable class labels for each of these images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tench', 'Tinca tinca'),\n",
       " ('English springer', 'English springer spaniel'),\n",
       " ('cassette player',),\n",
       " ('chain saw', 'chainsaw'),\n",
       " ('church', 'church building'),\n",
       " ('French horn', 'horn'),\n",
       " ('garbage truck', 'dustcart'),\n",
       " ('gas pump', 'gasoline pump', 'petrol pump', 'island dispenser'),\n",
       " ('golf ball',),\n",
       " ('parachute', 'chute')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenette_training.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert `training_class_indices` to human readable `training_labels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['French horn', 'French horn', 'French horn']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_class = [c[0] for c in imagenette_training.classes]\n",
    "training_labels = [index_to_class[y] for y in training_class_indices]\n",
    "training_labels[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how many images are associated with each of these 10 labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gas pump            419\n",
       "church              409\n",
       "golf ball           399\n",
       "English springer    395\n",
       "French horn         394\n",
       "parachute           390\n",
       "garbage truck       389\n",
       "tench               387\n",
       "chain saw           386\n",
       "cassette player     357\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(training_labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, roughly 350 to 420 images are there in each of these 10 categories.\n",
    "\n",
    "Now, we are ready to generate embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're using a [CLIP model](https://github.com/openai/CLIP) to analyze the images to generate embeddings or numerical representations of images. \n",
    "\n",
    "The \"ViT-B/16\" model here refers to a Vision Transformer model with an embedding size of 512 dimensions. Given an image or text passage, this model outputs a 512-dimensional vector representing the data. Images or passages with similar content should have embedding vectors with a high cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelParameterCount': '149,620,737',\n",
       " 'inputResolution': 224,\n",
       " 'contextLength': 77,\n",
       " 'vocabSize': 49408}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"modelParameterCount\": f'{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}',\n",
    " \"inputResolution\" : model.visual.input_resolution,\n",
    " \"contextLength\" : model.context_length,\n",
    " \"vocabSize\": model.vocab_size\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specific model is chosen here because it provides a good balance between performance and computational efficiency for many image analysis tasks. It is not the best in terms of accuracy, but allows us to introduce some aspects of Cobalt UI that users will find useful. \n",
    "\n",
    "There are many models available in the CLIP model family, each with different architectural configurations and capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use some of these models in more advanced examples in later examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each of these images we generate an embedding, and then we stack all of those embeddings together vertically to create an embedding matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3925/3925 [01:13<00:00, 53.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate through training_images_paths to build an embedding of shape (N x 512).\n",
    "# This may take about 1-3 minutes depending on your machine.\n",
    "embedding_training = []\n",
    "for p in tqdm(training_images_paths):\n",
    "    with torch.no_grad():\n",
    "        image = preprocess(Image.open(p)).unsqueeze(0).to(device)\n",
    "        image_features = model.encode_image(image)\n",
    "        embedding_training.append(image_features)\n",
    "\n",
    "embedding_np_training = [element.cpu().numpy() for element in embedding_training]\n",
    "embedding_matrix_training = np.concatenate(embedding_np_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3925, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_training.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows in the embedding matrix above is the number of images, and the number of columns is the number of embedding dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to explore how Cobalt groups the Imagenette dataset based on these CLIP embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cobalt Workspace and UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load this metadata into cobalt:\n",
    "1. *Image labels*: We add image labels so that we can see if the CLIP embeddings are consistent with the target labels of the dataset.\n",
    "2. *The embedding matrix*: We use `add_embedding_array` to add an embedding. Note that every element in your dataframe needs to have a corresponding row in your embedding.\n",
    "3. *Raw image file paths*: We're going to need to perform one additional step of `add_media_column` to pass in a list of training_images_paths that should be interpreted as images for Cobalt to display. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .v-expansion-panel::before {\n",
       "                box-shadow: none !important;\n",
       "            }\n",
       "            .v-expansion-panel-content__wrap {\n",
       "                padding: 0 !important;\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            #notebook-container {\n",
       "                width: 85% !important;\n",
       "            }\n",
       "\n",
       "            body[data-notebook='notebooks'] .jp-WindowedPanel-outer {\n",
       "                padding-left: 10% !important;\n",
       "                padding-right: 10% !important;\n",
       "            }\n",
       "\n",
       "            #app {\n",
       "                position: relative;\n",
       "            }\n",
       "\n",
       "            .v-dialog__content {\n",
       "                position: absolute !important;\n",
       "            }\n",
       "\n",
       "            :root {\n",
       "                --v-textBackground: #E3E4E7;\n",
       "            }\n",
       "\n",
       "            .theme--dark {\n",
       "                --v-textBackground: #696969;\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1. Create a pandas data frame with reference labels\n",
    "df = pd.DataFrame({\"targets\": training_labels})\n",
    "df[\"targets\"] = df[\"targets\"].astype(\"category\")\n",
    "\n",
    "# Step 2. Convert the data frame to CobaltDataset and add embedding array and image paths.\n",
    "ds = cobalt.CobaltDataset(df)\n",
    "ds.add_embedding_array(embedding_matrix_training)\n",
    "ds.add_media_column(training_images_paths, local_root_path=imagenette_training.root)\n",
    "\n",
    "# Step 3. Create a workspace with the CobaltDataset\n",
    "w = cobalt.Workspace(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open the UI and see how Cobalt helps understand the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.ui.table_image_size = (160, 160)\n",
    "w.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the groups that we had manually selected above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'twin_group': CobaltDataSubset(source_dataset='Unnamed Dataset',\n",
       " columns=['targets', '_image_path'],\n",
       " media_columns=['_image_path'],\n",
       " timestamp_columns=[],\n",
       " hidable_columns=[],\n",
       " other_metadata_columns=[]\n",
       " )}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.get_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the following cell, please make sure to create a group in the UI named 'twin_group' (or change the name of the group in the command below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w.get_groups()['twin_group'].df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data in the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different classes seem to be well separated in the graph shown above. You can double-click on nodes of the graph to select them, and open the data table to see the data contained in the selected node(s). \n",
    "\n",
    "You can also explore the automatically-generated clusters and see how well they align with the target classes. Each cluster seems to correspond to one class, but some classes are split into multiple clusters. See if you can come up with a hypothesis for why this might be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to access the analysis results and algorithms without using the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the groups that default settings of Cobalt found above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = w.clustering_results[\"auto_cluster\"]\n",
    "subgroups = [g.subset for g in clusters.groups]\n",
    "len(subgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `subgroups` is a list of all of the clusters that Cobalt found.\n",
    "\n",
    "Let's look at the data in the first subgroup, as a `pd.DataFrame` by running: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>targets</th>\n",
       "      <th>_image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>French horn</td>\n",
       "      <td>imagenette2/val/n03394916/n03394916_33711.JPEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>French horn</td>\n",
       "      <td>imagenette2/val/n03394916/n03394916_35731.JPEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>French horn</td>\n",
       "      <td>imagenette2/val/n03394916/n03394916_19491.JPEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>French horn</td>\n",
       "      <td>imagenette2/val/n03394916/n03394916_33380.JPEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>garbage truck</td>\n",
       "      <td>imagenette2/val/n03417042/n03417042_1271.JPEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3716</th>\n",
       "      <td>cassette player</td>\n",
       "      <td>imagenette2/val/n02979186/n02979186_14462.JPEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>cassette player</td>\n",
       "      <td>imagenette2/val/n02979186/n02979186_10402.JPEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3764</th>\n",
       "      <td>cassette player</td>\n",
       "      <td>imagenette2/val/n02979186/n02979186_13740.JPEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3857</th>\n",
       "      <td>cassette player</td>\n",
       "      <td>imagenette2/val/n02979186/n02979186_18940.JPEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3867</th>\n",
       "      <td>cassette player</td>\n",
       "      <td>imagenette2/val/n02979186/n02979186_13510.JPEG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              targets                                     _image_path\n",
       "67        French horn  imagenette2/val/n03394916/n03394916_33711.JPEG\n",
       "238       French horn  imagenette2/val/n03394916/n03394916_35731.JPEG\n",
       "298       French horn  imagenette2/val/n03394916/n03394916_19491.JPEG\n",
       "354       French horn  imagenette2/val/n03394916/n03394916_33380.JPEG\n",
       "439     garbage truck   imagenette2/val/n03417042/n03417042_1271.JPEG\n",
       "...               ...                                             ...\n",
       "3716  cassette player  imagenette2/val/n02979186/n02979186_14462.JPEG\n",
       "3741  cassette player  imagenette2/val/n02979186/n02979186_10402.JPEG\n",
       "3764  cassette player  imagenette2/val/n02979186/n02979186_13740.JPEG\n",
       "3857  cassette player  imagenette2/val/n02979186/n02979186_18940.JPEG\n",
       "3867  cassette player  imagenette2/val/n02979186/n02979186_13510.JPEG\n",
       "\n",
       "[423 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgroups[0].df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can see the images in it by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.view_table(subgroups[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to explore and try out more things!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details about the UI can be found here: [https://docs.cobalt.bluelightai.com/ui.html](https://docs.cobalt.bluelightai.com/ui.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cobalt gives a powerful way to interact and visualize complex data. \n",
    "- It is important to remember that the default settings used by Cobalt may not always perfectly separate classes, but it can provide a good starting point for further analysis. \n",
    "- The UI provides an easy way to explore and interact with the data, while also providing some basic visualizations.\n",
    "    - While UI is very helpful in generating insights and interactive data analysis, it is not the only way to use Cobalt.\n",
    "    - The API allows for more advanced usage and customization, such as automating workflows or integrating with other systems.\n",
    "- Check out other videos to see how Cobalt can be used for other applications.\n",
    "\n",
    "- Use Cobalt's API to automate more complex workflows or integrate it with other tools and systems.\n",
    "- Download and use it today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And do let us know what you think about it:\n",
    "<a href=\"https://bluelightai.com/contact\">Give Feedback</a> | <a href=\"https://bluelightai.com/\">Our Website</a> | <a href=\"https://bluelightai.com/blog\">Our Blog</a> | <a href=\"https://docs.cobalt.bluelightai.com/\">Cobalt Docs</a> | <a href=\"https://join.slack.com/t/bluelightaicom/shared_invite/zt-2uj0iu5lh-5WgutuwH82RxAOwuq8ptqg\">Slack Community</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: space-between;\">\n",
    "    <div style:\"flex: 1; text-align: left;\">\n",
    "        <a href=\"#top\" style=\"text-decoration: none; color: inherit;\"> \n",
    "            <h3>Top of Page</h3> \n",
    "        </a>\n",
    "    </div>\n",
    "    <div style:\"flex: 1; text-align: right;\">\n",
    "        <img style=\"width:50%;max-width:600px;float:right\" alt=\"Bluelight AI Logo\" href=\"https://bluelightai.com/\" src=\"https://github.com/BlueLightAI/cobalt-examples/blob/main/assets/blai-logo-light.png?raw=true\">\n",
    "    </div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_co_pypi_3_12_7_20241202",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
